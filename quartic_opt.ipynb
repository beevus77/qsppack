{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ff0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import conv1d, pad\n",
    "from torch.fft import fft\n",
    "from torchaudio.transforms import FFTConvolve\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df692f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69066987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_torch(a, b, P):\n",
    "    \"\"\"\n",
    "    Computes the loss for the optimization problem.\n",
    "\n",
    "    This function calculates the loss as the squared norm of the difference\n",
    "    between the target tensor P and the convolution of x with its flipped version.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : torch.Tensor\n",
    "        The first input tensor for which the loss is computed.\n",
    "    b : torch.Tensor\n",
    "        The second input tensor for which the loss is computed.\n",
    "    P : torch.Tensor\n",
    "        The target tensor to compare against.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The computed loss value.\n",
    "    \"\"\"\n",
    "    a.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "\n",
    "    # Compute loss using squared distance function\n",
    "    loss = torch.norm(FFTConvolve(\"full\").forward(a, torch.flip(a, dims=[0])) - FFTConvolve(\"full\").forward(b, torch.flip(b, dims=[0])))**2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def closure():\n",
    "    \"\"\"\n",
    "    Closure function for the optimizer.\n",
    "\n",
    "    This function zeroes the gradients, computes the loss using the objective_torch\n",
    "    function, and performs backpropagation to compute the gradients.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The computed loss value.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    loss = objective_torch(initial, conv_p_negative)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Set the size of the polynomial\n",
    "N = 16\n",
    "\n",
    "# Generate a random polynomial of size N on the specified device\n",
    "poly = torch.randn(N, device=device)\n",
    "\n",
    "# Define the granularity for padding\n",
    "granularity = 2 ** 25\n",
    "\n",
    "# Pad the polynomial to match the granularity\n",
    "P = pad(poly, (0, granularity - poly.shape[0]))\n",
    "\n",
    "# Compute the FFT of the padded polynomial\n",
    "ft = fft(P)\n",
    "\n",
    "# Normalize the polynomial using the maximum norm of its FFT\n",
    "P_norms = ft.abs()\n",
    "poly /= torch.max(P_norms)\n",
    "\n",
    "# Compute the negative convolution of the polynomial with its flipped version\n",
    "conv_p_negative = FFTConvolve(\"full\").forward(poly, torch.flip(poly, dims=[0])) * -1\n",
    "\n",
    "# Adjust the last element to ensure the norm condition\n",
    "conv_p_negative[poly.shape[0] - 1] = 1 - torch.norm(poly) ** 2\n",
    "\n",
    "# Set up optimizer\n",
    "initial = torch.randn(poly.shape[0], device=device, requires_grad=True)\n",
    "initial = (initial / torch.norm(initial)).clone().detach().requires_grad_(True)\n",
    "optimizer = torch.optim.LBFGS([initial], max_iter=1000)\n",
    "\n",
    "# Perform the optimization step using the closure function and record the time\n",
    "t0 = time.time()\n",
    "optimizer.step(closure)\n",
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "\n",
    "# Print the results of the optimization\n",
    "print(f'N: {N}')\n",
    "print(f'Time: {total}')\n",
    "print(f'Final: {closure().item()}')\n",
    "print(f\"# Iterations: {optimizer.state[optimizer._params[0]]['n_iter']}\")\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cd8c76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 16\n",
      "Time: 0.6851620674133301\n",
      "Final: 5.608651321153957e-10\n",
      "# Iterations: 131\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def objective_torch(x, P):\n",
    "    \"\"\"\n",
    "    Computes the loss for the optimization problem.\n",
    "\n",
    "    This function calculates the loss as the squared norm of the difference\n",
    "    between the target tensor P and the convolution of x with its flipped version.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        The input tensor for which the loss is computed.\n",
    "    P : torch.Tensor\n",
    "        The target tensor to compare against.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The computed loss value.\n",
    "    \"\"\"\n",
    "    x.requires_grad = True\n",
    "\n",
    "    # Compute loss using squared distance function\n",
    "    loss = torch.norm(P - FFTConvolve(\"full\").forward(x, torch.flip(x, dims=[0])))**2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def closure():\n",
    "    \"\"\"\n",
    "    Closure function for the optimizer.\n",
    "\n",
    "    This function zeroes the gradients, computes the loss using the objective_torch\n",
    "    function, and performs backpropagation to compute the gradients.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The computed loss value.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    loss = objective_torch(initial, conv_p_negative)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Set the size of the polynomial\n",
    "N = 16\n",
    "\n",
    "# Generate a random polynomial of size N on the specified device\n",
    "poly = torch.randn(N, device=device)\n",
    "\n",
    "# Define the granularity for padding\n",
    "granularity = 2 ** 25\n",
    "\n",
    "# Pad the polynomial to match the granularity\n",
    "P = pad(poly, (0, granularity - poly.shape[0]))\n",
    "\n",
    "# Compute the FFT of the padded polynomial\n",
    "ft = fft(P)\n",
    "\n",
    "# Normalize the polynomial using the maximum norm of its FFT\n",
    "P_norms = ft.abs()\n",
    "poly /= torch.max(P_norms)\n",
    "\n",
    "# Compute the negative convolution of the polynomial with its flipped version\n",
    "conv_p_negative = FFTConvolve(\"full\").forward(poly, torch.flip(poly, dims=[0])) * -1\n",
    "\n",
    "# Adjust the last element to ensure the norm condition\n",
    "conv_p_negative[poly.shape[0] - 1] = 1 - torch.norm(poly) ** 2\n",
    "\n",
    "# Set up optimizer\n",
    "initial = torch.randn(poly.shape[0], device=device, requires_grad=True)\n",
    "initial = (initial / torch.norm(initial)).clone().detach().requires_grad_(True)\n",
    "optimizer = torch.optim.LBFGS([initial], max_iter=1000)\n",
    "\n",
    "# Perform the optimization step using the closure function and record the time\n",
    "t0 = time.time()\n",
    "optimizer.step(closure)\n",
    "t1 = time.time()\n",
    "total = t1 - t0\n",
    "\n",
    "# Print the results of the optimization\n",
    "print(f'N: {N}')\n",
    "print(f'Time: {total}')\n",
    "print(f'Final: {closure().item()}')\n",
    "print(f\"# Iterations: {optimizer.state[optimizer._params[0]]['n_iter']}\")\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d26cfd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 16\n",
      "Time: 0.8362300395965576\n",
      "Final: 6.212476089118013e-10\n",
      "# Iterations: 78\n",
      "-----------------------------------------------------\n",
      "N: 32\n",
      "Time: 1.9712750911712646\n",
      "Final: 4.43686212747707e-06\n",
      "# Iterations: 360\n",
      "-----------------------------------------------------\n",
      "N: 64\n",
      "Time: 1.5400800704956055\n",
      "Final: 6.780034595976758e-07\n",
      "# Iterations: 302\n",
      "-----------------------------------------------------\n",
      "N: 128\n",
      "Time: 2.8109209537506104\n",
      "Final: 1.3475355444825254e-05\n",
      "# Iterations: 483\n",
      "-----------------------------------------------------\n",
      "N: 256\n",
      "Time: 6.483417987823486\n",
      "Final: 1.036808953358559e-06\n",
      "# Iterations: 1000\n",
      "-----------------------------------------------------\n",
      "N: 512\n",
      "Time: 4.48820686340332\n",
      "Final: 1.8909720438387012e-06\n",
      "# Iterations: 816\n",
      "-----------------------------------------------------\n",
      "N: 1024\n",
      "Time: 7.285927057266235\n",
      "Final: 1.737327579576231e-06\n",
      "# Iterations: 936\n",
      "-----------------------------------------------------\n",
      "N: 2048\n",
      "Time: 6.603424787521362\n",
      "Final: 2.289321628268226e-06\n",
      "# Iterations: 1000\n",
      "-----------------------------------------------------\n",
      "N: 4096\n",
      "Time: 11.562613010406494\n",
      "Final: 1.8432632487019873e-06\n",
      "# Iterations: 1000\n",
      "-----------------------------------------------------\n",
      "N: 8192\n",
      "Time: 19.354732036590576\n",
      "Final: 2.2711039946443634e-06\n",
      "# Iterations: 1000\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m     loss.backward()\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m optimizer.step(closure)\n\u001b[32m     43\u001b[39m t1 = time.time()\n\u001b[32m     45\u001b[39m total = t1-t0\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qspy-py311/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    380\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    382\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    383\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m out = func(*args, **kwargs)\n\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qspy-py311/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qspy-py311/lib/python3.11/site-packages/torch/optim/lbfgs.py:440\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_iter != max_iter:\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m         loss = \u001b[38;5;28mfloat\u001b[39m(closure())\n\u001b[32m    441\u001b[39m     flat_grad = \u001b[38;5;28mself\u001b[39m._gather_flat_grad()\n\u001b[32m    442\u001b[39m     opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qspy-py311/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mclosure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m():\n\u001b[32m     36\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     loss = objective_torch(initial, conv_p_negative)\n\u001b[32m     38\u001b[39m     loss.backward()\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mobjective_torch\u001b[39m\u001b[34m(x, P)\u001b[39m\n\u001b[32m      2\u001b[39m x.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Compute loss using squared distance function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m loss = torch.norm(P - FFTConvolve(\u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m).forward(x, torch.flip(x, dims=[\u001b[32m0\u001b[39m])))**\u001b[32m2\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qspy-py311/lib/python3.11/site-packages/torchaudio/transforms/_transforms.py:1946\u001b[39m, in \u001b[36mFFTConvolve.forward\u001b[39m\u001b[34m(self, x, y)\u001b[39m\n\u001b[32m   1935\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n\u001b[32m   1936\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1937\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   1938\u001b[39m \u001b[33;03m        x (torch.Tensor): First convolution operand, with shape `(..., N)`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1944\u001b[39m \u001b[33;03m        the leading dimensions match those of ``x`` and `L` is dictated by ``mode``.\u001b[39;00m\n\u001b[32m   1945\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1946\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.fftconvolve(x, y, mode=\u001b[38;5;28mself\u001b[39m.mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qspy-py311/lib/python3.11/site-packages/torchaudio/functional/functional.py:2280\u001b[39m, in \u001b[36mfftconvolve\u001b[39m\u001b[34m(x, y, mode)\u001b[39m\n\u001b[32m   2277\u001b[39m _check_convolve_mode(mode)\n\u001b[32m   2279\u001b[39m n = x.size(-\u001b[32m1\u001b[39m) + y.size(-\u001b[32m1\u001b[39m) - \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2280\u001b[39m fresult = torch.fft.rfft(x, n=n) * torch.fft.rfft(y, n=n)\n\u001b[32m   2281\u001b[39m result = torch.fft.irfft(fresult, n=n)\n\u001b[32m   2282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_convolve_mode(result, x.size(-\u001b[32m1\u001b[39m), y.size(-\u001b[32m1\u001b[39m), mode)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def objective_torch(x, P):\n",
    "    x.requires_grad = True\n",
    "\n",
    "    # Compute loss using squared distance function\n",
    "    loss = torch.norm(P - FFTConvolve(\"full\").forward(x, torch.flip(x, dims=[0])))**2\n",
    "    return loss\n",
    "\n",
    "times = []\n",
    "final_vals = []\n",
    "num_iterations = []\n",
    "\n",
    "for k in range(4, 20):\n",
    "    N = 2 ** k\n",
    "    poly = torch.randn(N, device=device)\n",
    "\n",
    "    granularity = 2 ** 25\n",
    "    P = pad(poly, (0, granularity - poly.shape[0]))\n",
    "    ft = fft(P)\n",
    "\n",
    "    # Normalize P\n",
    "    P_norms = ft.abs()\n",
    "    poly /= torch.max(P_norms)\n",
    "\n",
    "    conv_p_negative = FFTConvolve(\"full\").forward(poly, torch.flip(poly, dims=[0]))* -1\n",
    "    conv_p_negative[poly.shape[0] - 1] = 1 - torch.norm(poly) ** 2\n",
    "\n",
    "    # Initializing Q randomly to start with\n",
    "    initial = torch.randn(poly.shape[0], device=device, requires_grad=True)\n",
    "    initial = (initial / torch.norm(initial)).clone().detach().requires_grad_(True)\n",
    "\n",
    "    optimizer = torch.optim.LBFGS([initial], max_iter=1000)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = objective_torch(initial, conv_p_negative)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    total = t1-t0\n",
    "    times.append(total)\n",
    "    final_vals.append(closure().item())\n",
    "    num_iterations.append(optimizer.state[optimizer._params[0]]['n_iter'])\n",
    "    print(f'N: {N}')\n",
    "    print(f'Time: {total}')\n",
    "    print(f'Final: {closure().item()}')\n",
    "    print(f\"# Iterations: {optimizer.state[optimizer._params[0]]['n_iter']}\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(times)\n",
    "print(final_vals)\n",
    "print(num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qspy-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
